{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the code using In-hospital network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vitaldb\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "import os\n",
    "\n",
    "def create_safe_tuple_string(items):\n",
    "    \"\"\"Create a safe SQL IN clause string from a list of items.\"\"\"\n",
    "    items = [int(item) for item in items]\n",
    "    if len(items) == 1:\n",
    "        return f\"({items[0]})\"  # Single item\n",
    "    else:\n",
    "        return str(tuple(items))  # Multiple items\n",
    "\n",
    "# Database connection\n",
    "try:\n",
    "    db = pymysql.connect(\n",
    "        host='172.16.142.70',\n",
    "        port=3306,\n",
    "        user='vitaldb',\n",
    "        passwd='qkdlxkf2469',\n",
    "        db='snuop',\n",
    "        charset='utf8'\n",
    "    )\n",
    "    cur = db.cursor()\n",
    "\n",
    "    # Define target tracks\n",
    "    target_tracks = ('Root/EEG_L','Root/EEG_L1','Root/EEG_L2', 'Root/EEG_R','Root/EEG_R1','Root/EEG_R2')\n",
    "    \n",
    "    # Get track IDs\n",
    "    trkname_sql = f\"SELECT * FROM tracks WHERE tracks.trkname IN {str(target_tracks)};\"\n",
    "    cur.execute(trkname_sql)\n",
    "    data = cur.fetchall()\n",
    "    trkdf = pd.DataFrame(data, columns=['trkid', 'trkname'])\n",
    "    track_ids = trkdf['trkid'].values\n",
    "    trkname = list(trkdf['trkname'])\n",
    "\n",
    "    # Construct and execute the file list query\n",
    "    track_tuple_str = create_safe_tuple_string(track_ids)\n",
    "    filelst_sql = f\"\"\"\n",
    "        SELECT vitalfiles.filename, vitalfiles.dtstart, vitalfiles.dtend, vitalfiles_trk.trkid \n",
    "        FROM vitalfiles \n",
    "        INNER JOIN vitalfiles_trk ON vitalfiles_trk.fileid = vitalfiles.fileid \n",
    "        WHERE vitalfiles_trk.trkid IN {track_tuple_str}\n",
    "    \"\"\"\n",
    "    cur.execute(filelst_sql)\n",
    "    \n",
    "    data = cur.fetchall()\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data, columns=['filename', 'dtstart', 'dtend', 'trkid'])\n",
    "    \n",
    "    df.to_csv('eeg_list.csv')\n",
    "    print(f\"Successfully retrieved {len(df)} records\")\n",
    "    \n",
    "except pymysql.Error as e:\n",
    "    print(f\"Database error occurred: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    if 'db' in locals():\n",
    "        db.close()\n",
    "'''\n",
    "df = pd.read_csv('./eeg_list.csv')\n",
    "df['dtstart'] = pd.to_datetime(df['dtstart'])\n",
    "df['dtstart'].min(), df['dtstart'].max()\n",
    "'''\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv('./eeg_list.csv')\n",
    "df = df[~df['filename'].str.contains('CU', na=False)]\n",
    "df = df[~df['filename'].str.contains('P', na=False)]\n",
    "df['dtstart'] = pd.to_datetime(df['dtstart'])\n",
    "\n",
    "# VitalDB 로그인\n",
    "if vitaldb.login('eundain94', 'eundain94', '172.16.142.27', '80'):\n",
    "    DOWNLOAD_DIR = 'D:/data'\n",
    "    \n",
    "    # 디렉토리가 없으면 생성\n",
    "    if not os.path.exists(DOWNLOAD_DIR):\n",
    "        os.makedirs(DOWNLOAD_DIR)\n",
    "    \n",
    "    # DataFrame의 각 파일에 대해 다운로드 수행\n",
    "    for filename in df['filename'].unique():  # filename 열의 이름이 'filename'이라고 가정\n",
    "        opath = os.path.join(DOWNLOAD_DIR, filename)\n",
    "        try:\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            vitaldb.download(filename, opath)\n",
    "            print(f\"Successfully downloaded {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {filename}: {e}\")\n",
    "            continue\n",
    "    print(\"Download process completed\")\n",
    "else:\n",
    "    print(\"Failed to login to VitalDB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible to use any network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import os, shutil\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\안장호\\AppData\\Local\\Temp\\ipykernel_23476\\2844804582.py:6: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  x1 = pd.read_csv('adt.csv', usecols=['hid1', 'filename']) # downloaded from VitalDB ADT\n"
     ]
    }
   ],
   "source": [
    "vital_files =[]\n",
    "for file in os.listdir(\"D:/data\"):\n",
    "    if file.endswith(\".vital\"):\n",
    "        vital_files.append(file[:-6])\n",
    "\n",
    "x1 = pd.read_csv('adt.csv', usecols=['hid1', 'filename']) # downloaded from VitalDB ADT\n",
    "x1['hid1'] = x1['hid1'].astype(str).apply(lambda x: '0'+x if len(x)==7 else x)\n",
    "x1 = x1[x1['hid1'].str.match(r'^\\d{8}$')]\n",
    "x1= x1[x1['filename'].isin(vital_files)]\n",
    "\n",
    "x2= pd.read_csv('eeg_list.csv')\n",
    "dict_start = dict(zip(x2['filename'].str.replace('.vital',''),x2['dtstart']))\n",
    "dict_end = dict(zip(x2['filename'].str.replace('.vital',''),x2['dtend']))\n",
    "\n",
    "x1['dtstart'] = x1['filename'].map(dict_start)\n",
    "x1['dtend']=x1['filename'].map(dict_end)\n",
    "x1 = x1.rename(columns={'hid1':'hid'})\n",
    "x1 = x1[['hid','filename','dtstart','dtend']]\n",
    "x1.to_csv('eeg_hid.csv', index=False)\n",
    "# Make eeg_hid.xlsx from eeg_hid.csv, then upload to RidEx to get eeg_rid.csv\n",
    "# Leave only rid column (without header), remove blank rows, and upload to Supreme in '환자등록번호' section\n",
    "# Then download delirium.csv from Supreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oskum\\AppData\\Local\\Temp\\ipykernel_24348\\2300523585.py:9: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  x2['dtstart'] = pd.to_datetime(pd.to_datetime(x2['dtstart']).dt.date)\n",
      "Processing prescriptions: 100%|██████████| 35115/35115 [08:06<00:00, 72.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_p\n",
      "0    34675\n",
      "1      440\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking prior prescriptions: 100%|██████████| 35115/35115 [08:05<00:00, 72.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exclude_p\n",
      "0    34906\n",
      "1      209\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing CSTs: 100%|██████████| 35115/35115 [07:06<00:00, 82.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_c\n",
      "0    34987\n",
      "1      128\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking prior CSTs: 100%|██████████| 35115/35115 [07:01<00:00, 83.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exclude_c\n",
      "0    35055\n",
      "1       60\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    34283\n",
      "1      267\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x1 = pd.read_csv('delirium_old.csv', encoding='cp949', encoding_errors='ignore')\n",
    "x2 = pd.read_csv('eeg_rid.csv')\n",
    "\n",
    "x1['생년월일']= pd.to_datetime(x1['생년월일'])\n",
    "x1['입원일자']= pd.to_datetime(x1['입원일자'])\n",
    "x1['퇴원일자']= pd.to_datetime(x1['퇴원일자'])\n",
    "x1['약품처방일']= pd.to_datetime(x1['약품처방일'])\n",
    "x1['서식작성일']= pd.to_datetime(x1['서식작성일'])\n",
    "x2['dtstart'] = pd.to_datetime(pd.to_datetime(x2['dtstart']).dt.date)\n",
    "\n",
    "x1['label_c']=0\n",
    "x2['label_p']=0\n",
    "x2['exclude_p']=0\n",
    "x2['label_c']=0\n",
    "x2['exclude_c']=0\n",
    "\n",
    "for idx, row in tqdm(x2.iterrows(), total=len(x2), desc=\"Processing prescriptions\"):\n",
    "    matching_rows = x1[x1['연구별 환자 ID']==row['hid']]\n",
    "    has_prescription = matching_rows[\n",
    "        #(matching_rows['약품명(성분명)'].str.contains('haloperidol', case=False, na=False)) &\n",
    "        (matching_rows['입원일자']<=row['dtstart']) &        \n",
    "        (row['dtstart']<=matching_rows['약품처방일']) &\n",
    "        (matching_rows['약품처방일']<=matching_rows['퇴원일자']) \n",
    "        #(matching_rows['약품처방일']<=(row['dtstart']+pd.DateOffset(days=7)))\n",
    "    ].shape[0] >0\n",
    "    x2.loc[idx, 'label_p']=1 if has_prescription else 0\n",
    "    if has_prescription:\n",
    "        # Find minimum date among only positive prescription dates\n",
    "        positive_dates = matching_rows[matching_rows['약품처방일'] > row['dtstart']]['약품처방일']\n",
    "        if not positive_dates.empty:\n",
    "            x2.loc[idx,'label_p_interval'] = int((positive_dates.min() - row['dtstart']).days)\n",
    "print(x2['label_p'].value_counts())\n",
    "\n",
    "for idx, row in tqdm(x2.iterrows(), total=len(x2), desc=\"Checking prior prescriptions\"):\n",
    "    matching_rows = x1[x1['연구별 환자 ID']==row['hid']]\n",
    "    has_prescription_before = matching_rows[\n",
    "        #(matching_rows['약품명(성분명)'].str.contains('haloperidol', case=False, na=False)) &\n",
    "        (matching_rows['입원일자']<=matching_rows['약품처방일']) &\n",
    "        (matching_rows['약품처방일']<=row['dtstart']) &\n",
    "        (row['dtstart']<=matching_rows['퇴원일자'])\n",
    "    ].shape[0] >0\n",
    "    if has_prescription_before:\n",
    "        x2.loc[idx, 'exclude_p']=1\n",
    "print(x2['exclude_p'].value_counts())\n",
    "\n",
    "patterns = [\n",
    "    r'A\\>.*?delirium.*?P\\>',\n",
    "    r'A\\>.*?delirium.*?Rec\\>',\n",
    "    r'Ass\\>.*?delirium.*?Rec\\>'\n",
    "]\n",
    "x1['label_c'] = x1['서식내용'].fillna('').str.contains(\n",
    "    '|'.join(patterns), \n",
    "    case=False, \n",
    "    regex=True,\n",
    "    flags=re.DOTALL  # Allow matching across newlines\n",
    ").astype(int)\n",
    "for idx, row in tqdm(x2.iterrows(), total=len(x2), desc=\"Processing CSTs\"):\n",
    "    matching_rows = x1[x1['연구별 환자 ID']==row['hid']]\n",
    "    has_prescription = matching_rows[\n",
    "        (matching_rows['label_c']==1) &\n",
    "        (matching_rows['입원일자']<=row['dtstart']) &        \n",
    "        (row['dtstart']<=matching_rows['서식작성일']) &\n",
    "        (matching_rows['서식작성일']<=matching_rows['퇴원일자'])\n",
    "        #(matching_rows['서식작성일']<=(row['dtstart']+pd.DateOffset(days=7)))\n",
    "    ].shape[0] >0\n",
    "    x2.loc[idx, 'label_c']=1 if has_prescription else 0\n",
    "    if has_prescription:\n",
    "        positive_dates = matching_rows[matching_rows['서식작성일'] > row['dtstart']]['서식작성일']\n",
    "        if not positive_dates.empty:\n",
    "            x2.loc[idx,'label_c_interval'] = int((positive_dates.min() - row['dtstart']).days)\n",
    "print(x2['label_c'].value_counts())\n",
    "\n",
    "for idx, row in tqdm(x2.iterrows(), total=len(x2), desc=\"Checking prior CSTs\"):\n",
    "    matching_rows = x1[x1['연구별 환자 ID']==row['hid']]\n",
    "    has_prescription_before = matching_rows[\n",
    "        (matching_rows['label_c']==1) &\n",
    "        #(matching_rows['약품명(성분명)'].str.contains('haloperidol', case=False, na=False)) &\n",
    "        (matching_rows['입원일자']<=matching_rows['서식작성일']) &\n",
    "        (matching_rows['서식작성일']<=row['dtstart']) &\n",
    "        (row['dtstart']<=matching_rows['퇴원일자'])\n",
    "    ].shape[0] >0\n",
    "    if has_prescription_before:\n",
    "        x2.loc[idx, 'exclude_c']=1\n",
    "print(x2['exclude_c'].value_counts())\n",
    "x2.to_csv('eeg_rid_supreme_labelled_raw.csv', index=False, encoding = 'utf-8-sig')\n",
    "\n",
    "x3 = x2[(x2['exclude_p']==0)&(x2['exclude_c']==0)]\n",
    "x3 = x3.drop(['exclude_p','exclude_c'],axis=1)\n",
    "\n",
    "birth_date_map = x1.set_index('연구별 환자 ID')['생년월일'].to_dict()\n",
    "x3['생년월일'] = x3['hid'].map(birth_date_map)\n",
    "x3['age'] = (x3['dtstart']-x3['생년월일']).dt.days/365.25\n",
    "x3 = x3[x3['age']>=18]\n",
    "\n",
    "# Create a combined label column\n",
    "# If either label_p or label_c is 1, set label to 1, otherwise 0\n",
    "x3['label'] = ((x3['label_p'] == 1) | (x3['label_c'] == 1)).astype(int)\n",
    "# Check the distribution of the new label column\n",
    "\n",
    "print(x3['label'].value_counts())\n",
    "x3.to_csv('eeg_rid_supreme_labelled.csv', index=False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train files: 31043\n",
      "Number of test files: 3507\n",
      "\n",
      "Train set label distribution:\n",
      "label\n",
      "0    30802\n",
      "1      241\n",
      "Name: count, dtype: int64\n",
      "Positive ratio: 0.008\n",
      "\n",
      "Test set label distribution:\n",
      "label\n",
      "0    3481\n",
      "1      26\n",
      "Name: count, dtype: int64\n",
      "Positive ratio: 0.007\n"
     ]
    }
   ],
   "source": [
    "x3 = pd.read_csv('eeg_rid_supreme_labelled.csv')\n",
    "# Get unique HIDs and their corresponding filenames\n",
    "hid_to_files = x3.groupby('hid')['filename'].unique().to_dict()\n",
    "# Get unique HIDs\n",
    "unique_hids = list(hid_to_files.keys())\n",
    "# Split HIDs into train/test\n",
    "train_hids, test_hids = train_test_split(unique_hids, test_size=0.1, random_state=1)\n",
    "# Get filenames for train and test sets\n",
    "train_files = []\n",
    "for hid in train_hids:\n",
    "    train_files.extend(hid_to_files[hid])\n",
    "train_files = list(set(train_files))  # Remove any duplicates\n",
    "test_files = []\n",
    "for hid in test_hids:\n",
    "    test_files.extend(hid_to_files[hid])\n",
    "test_files = list(set(test_files))  # Remove any duplicates\n",
    "\n",
    "# Save train and test filename lists\n",
    "with open('train_files.pkl', 'wb') as f:\n",
    "    pickle.dump(train_files, f)\n",
    "with open('test_files.pkl', 'wb') as f:\n",
    "    pickle.dump(test_files, f)\n",
    "\n",
    "print(f\"Number of train files: {len(train_files)}\")\n",
    "print(f\"Number of test files: {len(test_files)}\")\n",
    "\n",
    "# Check label distribution in train and test sets\n",
    "train_labels = x3[x3['filename'].isin(train_files)]['label'].value_counts()\n",
    "test_labels = x3[x3['filename'].isin(test_files)]['label'].value_counts()\n",
    "\n",
    "print(\"\\nTrain set label distribution:\")\n",
    "print(train_labels)\n",
    "print(f\"Positive ratio: {train_labels[1]/(train_labels[0] + train_labels[1]):.3f}\")\n",
    "\n",
    "print(\"\\nTest set label distribution:\") \n",
    "print(test_labels)\n",
    "print(f\"Positive ratio: {test_labels[1]/(test_labels[0] + test_labels[1]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = pd.read_csv('eeg_rid_supreme_labelled.csv')\n",
    "with open('train_files.pkl', 'rb') as f:\n",
    "    train_files = pickle.load(f)\n",
    "with open('test_files.pkl', 'rb') as f:\n",
    "    test_files = pickle.load(f)\n",
    "\n",
    "test_data = x3[x3['filename'].isin(test_files)]\n",
    "test_pos = test_data[test_data['label'] == 1]\n",
    "test_neg = test_data[test_data['label'] == 0]\n",
    "\n",
    "os.makedirs('D:/test', exist_ok=True)\n",
    "test = x3.loc[x3['filename'].isin(test_files), 'filename'].unique()\n",
    "for filename in test:\n",
    "    src = os.path.join('D:/data',f'{filename}.vital')\n",
    "    dst = os.path.join('D:/test', f'{filename}.vital')\n",
    "    if os.path.exists(src) and os.path.getsize(src) > 0:\n",
    "        shutil.copy2(src,dst)\n",
    "#####################################################################################\n",
    "train_data = x3[x3['filename'].isin(train_files)]\n",
    "# Separate into positive and negative samples\n",
    "train_pos = train_data[train_data['label'] == 1]\n",
    "train_neg = train_data[train_data['label'] == 0]\n",
    "\n",
    "n_pos = len(train_pos['filename'].unique())\n",
    "n_neg = 5*n_pos  # We want twice as many negatives as positives\n",
    "# Randomly sample negative cases\n",
    "train_neg_sampled = train_neg.drop_duplicates('filename').sample(n=n_neg, random_state=42)\n",
    "# Combine positive and sampled negative cases\n",
    "train_5 = pd.concat([train_pos, train_neg_sampled])\n",
    "train_5 = train_5['filename'].unique()\n",
    "os.makedirs('D:/train_5', exist_ok=True)\n",
    "for filename in train_5:\n",
    "    src = os.path.join('D:/data',f'{filename}.vital')\n",
    "    dst = os.path.join('D:/train_5', f'{filename}.vital')\n",
    "    if os.path.exists(src) and os.path.getsize(src) > 0:\n",
    "        shutil.copy2(src,dst)\n",
    "########################################################################################\n",
    "x3[x3['filename'].isin(test)].to_csv('eeg_rid_supreme_test.csv', index=False)\n",
    "x3[x3['filename'].isin(train_5)].to_csv('eeg_rid_supreme_train_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('eeg_rid_supreme_test.csv')\n",
    "train_data = pd.read_csv('eeg_rid_supreme_train_5.csv')\n",
    "eeg_rid_supreme_sampled = pd.concat([test_data, train_data], ignore_index=True)\n",
    "eeg_rid_supreme_sampled.to_csv('eeg_rid_supreme_sampled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_rid_supreme_test = pd.read_csv('eeg_rid_supreme_test.csv')\n",
    "\n",
    "hid_grouped = eeg_rid_supreme_test.groupby('hid')['label'].agg(lambda x: x.value_counts().index[0]).reset_index()\n",
    "eeg_rid_supreme_test_unique_hid = eeg_rid_supreme_test.drop_duplicates('hid')\n",
    "eeg_rid_supreme_test_unique_hid = eeg_rid_supreme_test_unique_hid.merge(hid_grouped, on='hid', how='left', suffixes=('_orig', ''))\n",
    "\n",
    "positive_hids = eeg_rid_supreme_test_unique_hid[eeg_rid_supreme_test_unique_hid['label'] == 1]['hid'].values\n",
    "negative_hids = eeg_rid_supreme_test_unique_hid[eeg_rid_supreme_test_unique_hid['label'] == 0]['hid'].values\n",
    "np.random.seed(2)\n",
    "\n",
    "if len(negative_hids) > len(positive_hids):\n",
    "    negative_hids_sampled = np.random.choice(negative_hids, size=len(positive_hids), replace=False)\n",
    "else:\n",
    "    negative_hids_sampled = negative_hids\n",
    "test_ids_1_new = np.concatenate([positive_hids, negative_hids_sampled])\n",
    "np.save('test_ids_1.npy', test_ids_1_new, allow_pickle=True)\n",
    "\n",
    "if len(negative_hids) > len(positive_hids):\n",
    "    negative_hids_sampled = np.random.choice(negative_hids, size=len(positive_hids)*2, replace=False)\n",
    "else:\n",
    "    negative_hids_sampled = negative_hids\n",
    "test_ids_2_new = np.concatenate([positive_hids, negative_hids_sampled])\n",
    "np.save('test_ids_2.npy', test_ids_2_new, allow_pickle=True)\n",
    "\n",
    "eeg_rid_supreme_train_5 = pd.read_csv('eeg_rid_supreme_train_5.csv')\n",
    "\n",
    "np.save('test_ids.npy', eeg_rid_supreme_test['hid'].unique(), allow_pickle=True)\n",
    "np.save('train_ids_5.npy', eeg_rid_supreme_train_5['hid'].unique(), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2= pd.read_csv('delirium_old.csv', encoding='cp949', encoding_errors='ignore')\n",
    "x1= pd.read_csv('eeg_rid_supreme_sampled.csv')\n",
    "sex_mapping = dict(zip(x2['연구별 환자 ID'], x2['성별']))\n",
    "x1['sex'] = x1['hid'].map(sex_mapping)\n",
    "x1['sex'] = x1['sex'].map({'M': 1, 'F': 0})\n",
    "\n",
    "for idx in tqdm(range(len(x1))):\n",
    "    vf = vitaldb.VitalFile('D:/combined/'+x1.loc[idx,'filename']+'.vital', track_names=['ROOT/EEG_L1', 'ROOT/EEG_L2', 'ROOT/EEG_R1', 'ROOT/EEG_R2', 'ROOT/EEG_L', 'ROOT/EEG_R','ROOT/SR'])\n",
    "    vf.to_vital('D:/combined_2/'+x1.loc[idx,'filename']+'.vital')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "240305",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
